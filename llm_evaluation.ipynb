{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "lxNSDQiABtXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation with Vertex AI"
      ],
      "metadata": {
        "id": "fTMB2k0eOI8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLMOps, or Large Language Model Operations, is an important methodology as organizations increasingly adopt large language models (LLMs) for a wide range of applications. LLMOps is the set of tools, processes, and best practices for managing the lifecycle of LLMs, from development and deployment to monitoring and maintenance. Vertex AI offers services to manage LLMOps pipelines as also mechanisms to evaluate the new models quality after every pipeline execution that you run.\n",
        "\n",
        "In the realm of Generative AI, evaluation is a critical aspect of assessing the quality and relevance of the generated text. It involves examining the output from a generative language model to determine its coherence, accuracy, and alignment with the provided prompt. Model evaluation helps identify areas for improvement, optimize model performance, and ensure that the generated text meets the desired standards for quality and usefulness. Read more about it in the official [documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/models/evaluate-models)."
      ],
      "metadata": {
        "id": "F_NMgRbeOPqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objective"
      ],
      "metadata": {
        "id": "oYtyoQLFOjCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This lab teaches you how to evaluate a foundation model or a fine tuned model based on Automatic Metrics of model results on the evaluation data and you will use the following Google Cloud products:\n",
        "*  Vertex AI Pipelines\n",
        "*  Vertex AI Evaluation Services\n",
        "*  Vertex AI Model Registry\n",
        "*  Vertex AI Endpoints"
      ],
      "metadata": {
        "id": "sOADS4gWOiSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Case"
      ],
      "metadata": {
        "id": "NsFmnmMTO5ur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Generative AI we will evaluate a model that generates a suitable TITLE for a news BODY from BBC FULLTEXT DATA (Sourced from BigQuery Public Dataset *bigquery-public-data.bbc_news.fulltext*). The evaluation will cover both foundation (text-bison@002) and fine tuned (called \"bbc-news-summary-tuned\") models with the automatic metrics method."
      ],
      "metadata": {
        "id": "60Z0slRTO7XO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install and Import Dependencies"
      ],
      "metadata": {
        "id": "H0-hgKhKPR7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-aiplatform\n",
        "!pip install --user datasets\n",
        "!pip install --user google-cloud-pipeline-components"
      ],
      "metadata": {
        "id": "jjhtaEINPXqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1I1sIWPPYCH",
        "outputId": "e83d042b-325e-4c15-924c-f466169a9156"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import aiplatform\n",
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()"
      ],
      "metadata": {
        "id": "BqFpssQtPaEr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "PROJECT_ID = \"YOUR_PROJECT_ID\" #@param\n",
        "vertexai.init(project=PROJECT_ID)"
      ],
      "metadata": {
        "id": "BlHZtPG4PefN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "REGION = \"europe-west4\"\n",
        "project_id = \"YOUR_PROJECT_ID\""
      ],
      "metadata": {
        "id": "CjiKzWsIPf21"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! gcloud config set project {project_id}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KNt7En-PiMk",
        "outputId": "2f8c00b0-7d62-4425-97f0-2d3cc393e537"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the necessary libraries\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "import kfp\n",
        "import sys\n",
        "import uuid\n",
        "import json\n",
        "import vertexai\n",
        "import pandas as pd\n",
        "from google.auth import default\n",
        "from datasets import load_dataset\n",
        "from google.cloud import aiplatform\n",
        "from vertexai.preview.language_models import TextGenerationModel, EvaluationTextSummarizationSpec"
      ],
      "metadata": {
        "id": "my_iViPEPj5o"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare & Load Evaluation Data"
      ],
      "metadata": {
        "id": "47W9sNYaPvCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUCKET_NAME = 'img_public_test/next_demo'\n",
        "BUCKET_URI = f\"gs://img_public_test/next_demo/EVALUATE.jsonl\"\n",
        "REGION = \"europe-west4\""
      ],
      "metadata": {
        "id": "WKyKEwNRPpEM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_url = 'https://storage.googleapis.com/img_public_test/next_demo/EVALUATE.jsonl'\n",
        "df = pd.read_json(json_url, lines=True)\n",
        "print (df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5weG9pPZPxJD",
        "outputId": "1d229cac-c2e7-4b05-a16c-bf642737facd"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            input_text  \\\n",
            "0    Summarize this text to generate a title: A bro...   \n",
            "1    Summarize this text to generate a title: Ninte...   \n",
            "2    Summarize this text to generate a title: Gambl...   \n",
            "3    Summarize this text to generate a title: The n...   \n",
            "4    Summarize this text to generate a title: The t...   \n",
            "..                                                 ...   \n",
            "735  Summarize this text to generate a title: Film ...   \n",
            "736  Summarize this text to generate a title: R&B s...   \n",
            "737  Summarize this text to generate a title: Sir E...   \n",
            "738  Summarize this text to generate a title: Seneg...   \n",
            "739  Summarize this text to generate a title: Oscar...   \n",
            "\n",
            "                            output_text  \n",
            "0       US duo in first spam conviction  \n",
            "1      Nintendo DS makes its Euro debut  \n",
            "2     Rings of steel combat net attacks  \n",
            "3    What's next for next-gen consoles?  \n",
            "4     'Blog' picked as word of the year  \n",
            "..                                  ...  \n",
            "735    Bening makes awards breakthrough  \n",
            "736         Jamelia's return to the top  \n",
            "737   Elton plays Paris charity concert  \n",
            "738     Youssou N'Dour wins music prize  \n",
            "739        Vera Drake scoops film award  \n",
            "\n",
            "[740 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Fine Tuned Model"
      ],
      "metadata": {
        "id": "yO1Pbv0qijEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_model = TextGenerationModel.get_tuned_model(\"projects/273845608377/locations/europe-west4/models/4220809634753019904\")\n",
        "response = tuned_model.predict(\"Summarize this text to generate a title: \\n Ever noticed how plane seats appear to be getting smaller and smaller? With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk. They say that the shrinking space on aeroplanes is not only uncomfortable it it's putting our health and safety in danger. More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger? This week, a U.S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes, it doesn't stipulate a minimum amount of space for humans.\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjVWZiuMScXK",
        "outputId": "4e2f3c1d-1230-4034-8267-abbac2ee1f21"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Shrinking space on planes putting our health and safety in danger\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate of the Fine Tuned Model"
      ],
      "metadata": {
        "id": "50fnfpUriwhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Define the evaluation specification for a text summarization task on the fine tuned model\n",
        "task_spec = EvaluationTextSummarizationSpec(\n",
        "  task_name = \"summarization\",\n",
        "  ground_truth_data=df\n",
        ")"
      ],
      "metadata": {
        "id": "nU0BUihrUDgi"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "eval_metrics_finetuned = tuned_model.evaluate(task_spec=task_spec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRZEuGCYbUoi",
        "outputId": "0a2fe328-47ad-4386-96ec-5a4e78beb56b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/273845608377/locations/europe-west4/pipelineJobs/evaluation-llm-text-generation-pipeline-20240404000646\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/273845608377/locations/europe-west4/pipelineJobs/evaluation-llm-text-generation-pipeline-20240404000646')\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
            "https://console.cloud.google.com/vertex-ai/locations/europe-west4/pipelines/runs/evaluation-llm-text-generation-pipeline-20240404000646?project=273845608377\n",
            "INFO:vertexai.language_models._evaluatable_language_models:Your evaluation job is running and will take 15-20 minutes to complete. Click on the PipelineJob link to view progress.\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/273845608377/locations/europe-west4/pipelineJobs/evaluation-llm-text-generation-pipeline-20240404000646 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/273845608377/locations/europe-west4/pipelineJobs/evaluation-llm-text-generation-pipeline-20240404000646 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/273845608377/locations/europe-west4/pipelineJobs/evaluation-llm-text-generation-pipeline-20240404000646 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/273845608377/locations/europe-west4/pipelineJobs/evaluation-llm-text-generation-pipeline-20240404000646 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/273845608377/locations/europe-west4/pipelineJobs/evaluation-llm-text-generation-pipeline-20240404000646 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/273845608377/locations/europe-west4/pipelineJobs/evaluation-llm-text-generation-pipeline-20240404000646 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/273845608377/locations/europe-west4/pipelineJobs/evaluation-llm-text-generation-pipeline-20240404000646 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/273845608377/locations/europe-west4/pipelineJobs/evaluation-llm-text-generation-pipeline-20240404000646 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/273845608377/locations/europe-west4/pipelineJobs/evaluation-llm-text-generation-pipeline-20240404000646 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/273845608377/locations/europe-west4/pipelineJobs/evaluation-llm-text-generation-pipeline-20240404000646 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/273845608377/locations/europe-west4/pipelineJobs/evaluation-llm-text-generation-pipeline-20240404000646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(eval_metrics_finetuned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqtMnm9hiJYc",
        "outputId": "3d47aa59-39d1-4a92-ade5-3081fe64b061"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EvaluationMetric(bleu=None, rougeLSum=0.36600753600753694)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Base Model"
      ],
      "metadata": {
        "id": "q7hT9cv8in0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Create a reference to a generative AI model\n",
        "base_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
      ],
      "metadata": {
        "id": "R90QO6hfTXu6"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of the Base Model"
      ],
      "metadata": {
        "id": "2T6aTYGOi8c4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Define the evaluation specification for a text summarization task on the base model\n",
        "task_spec = EvaluationTextSummarizationSpec(\n",
        "  task_name = \"summarization\",\n",
        "  ground_truth_data=df\n",
        ")"
      ],
      "metadata": {
        "id": "GC3RLK_lcqEw"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "eval_metrics_base = base_model.evaluate(task_spec=task_spec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZUp255-c92t",
        "outputId": "69657530-30d8-4ab3-a356-3937cc08a258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/273845608377/locations/europe-west4/pipelineJobs/evaluation-llm-text-generation-pipeline-20240404005730\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/273845608377/locations/europe-west4/pipelineJobs/evaluation-llm-text-generation-pipeline-20240404005730')\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
            "https://console.cloud.google.com/vertex-ai/locations/europe-west4/pipelines/runs/evaluation-llm-text-generation-pipeline-20240404005730?project=273845608377\n",
            "INFO:vertexai.language_models._evaluatable_language_models:Your evaluation job is running and will take 15-20 minutes to complete. Click on the PipelineJob link to view progress.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(eval_metrics_base)"
      ],
      "metadata": {
        "id": "KqxGeGn1jBg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison"
      ],
      "metadata": {
        "id": "KM8u_6tujkqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see in the eval_metrics_finetuned and eval_metrics_base metrics of the Fine Tuned and Base models respectively, the Evaluation Metric is RELATIVELY higher for the Fine Tuned Model as it determines how the model should phrase  responses to your prompts:\n",
        "\n",
        "EvaluationMetric(bleu=None, rougeLSum=0.36600753600753694)\n",
        "\n",
        "**rougeLSum**: This is the ROUGE-L score for the summary. ROUGE-L is a recall-based metric that measures the overlap between a summary and a reference summary. It is calculated by taking the longest common subsequence (LCS) between the two summaries and dividing it by the length of the reference summary.\n",
        "\n",
        "The rougeLSum score in the given expression is 0.36600753600753694, which means that the summary has a 36.6% overlap with the reference summary."
      ],
      "metadata": {
        "id": "r8ay_kSyjF-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# View Evaluation Results in Cloud Storage and Console"
      ],
      "metadata": {
        "id": "Z2oU9z6cdsMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find the evaluation results in the Cloud Storage output directory that you specified when creating the evaluation job. The file is named evaluation_metrics.json.\n",
        "\n",
        "For tuned models, you can also view evaluation results in the Google Cloud console:\n",
        "\n",
        "In the Vertex AI section of the Google Cloud console, go to the Vertex AI Model Registry page.\n",
        "\n",
        "Click the name of the model that you want to view evaluation metrics for.\n",
        "\n",
        "In the Evaluate tab, click the name of the evaluation run that you want to view."
      ],
      "metadata": {
        "id": "1M44bbG-dxNz"
      }
    }
  ]
}